{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10336875,"sourceType":"datasetVersion","datasetId":6400780}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"48339e60","cell_type":"code","source":"from transformers import TapasTokenizer, TapasForQuestionAnswering\n\n# Load the model and tokenizer from the local directory\ntokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:18:29.073735Z","iopub.execute_input":"2024-12-31T18:18:29.074046Z","iopub.status.idle":"2024-12-31T18:18:46.643491Z","shell.execute_reply.started":"2024-12-31T18:18:29.074010Z","shell.execute_reply":"2024-12-31T18:18:46.642386Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/490 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b7cc8d974594663a2234820d01fdff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/262k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97612fab4f048caaa13447be83aefcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/154 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daad4142eb9a4c7a91de505b4fbf99d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d77c0f854841d3b75e6d7a3ccdfc2f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dea0fcd4414b3ba2cf20ff1f5ab1d5"}},"metadata":{}}],"execution_count":1},{"id":"f2fff661","cell_type":"code","source":"import chardet\nimport pandas as pd\n# Detect the encoding of the file\nwith open(\"/kaggle/input/orders-data-visualisation/orders.csv\", \"rb\") as f:\n    result = chardet.detect(f.read())\n\n# Read the CSV file with the detected encoding\norder_data = pd.read_csv(\"/kaggle/input/orders-data-visualisation/orders.csv\", encoding=result['encoding'])\n\n# Display the first few rows of the DataFrame to verify\nprint(order_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:51:05.836496Z","iopub.execute_input":"2024-12-31T18:51:05.836892Z","iopub.status.idle":"2024-12-31T18:51:13.798867Z","shell.execute_reply.started":"2024-12-31T18:51:05.836862Z","shell.execute_reply":"2024-12-31T18:51:13.797854Z"}},"outputs":[{"name":"stdout","text":"   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n0       1  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n1       2  CA-2018-152156   11/8/2018  11/11/2018    Second Class    CG-12520   \n2       3  CA-2018-138688   6/12/2018   6/16/2018    Second Class    DV-13045   \n3       4  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n4       5  US-2017-108966  10/11/2017  10/18/2017  Standard Class    SO-20335   \n\n     Customer Name    Segment Country/Region             City  ...  \\\n0      Claire Gute   Consumer  United States        Henderson  ...   \n1      Claire Gute   Consumer  United States        Henderson  ...   \n2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n\n  Postal Code  Region       Product ID         Category Sub-Category  \\\n0     42420.0   South  FUR-BO-10001798        Furniture    Bookcases   \n1     42420.0   South  FUR-CH-10000454        Furniture       Chairs   \n2     90036.0    West  OFF-LA-10000240  Office Supplies       Labels   \n3     33311.0   South  FUR-TA-10000577        Furniture       Tables   \n4     33311.0   South  OFF-ST-10000760  Office Supplies      Storage   \n\n                                        Product Name     Sales  Quantity  \\\n0                  Bush Somerset Collection Bookcase  261.9600         2   \n1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n\n   Discount    Profit  \n0      0.00   41.9136  \n1      0.00  219.5820  \n2      0.00    6.8714  \n3      0.45 -383.0310  \n4      0.20    2.5164  \n\n[5 rows x 21 columns]\n","output_type":"stream"}],"execution_count":45},{"id":"455346c9","cell_type":"code","source":"# Ask Questions\ninputs = tokenizer(\n    table=grouped_data,\n    queries=[\"What segment has the highest sales?\"],\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\noutputs = model(**inputs)\n\n# Process the outputs to get the answer\nlogits = outputs.logits\npredicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\nprint(f\"Predicted answer: {predicted_answer}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"59f63600","cell_type":"code","source":"import os\nos.listdir()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"data":{"text/plain":["['.ipynb_checkpoints',\n"," 'BERT',\n"," 'BERT.ipynb',\n"," 'fine-tuned-tapas',\n"," 'fine_tuned_tabert',\n"," 'fraudTest.csv',\n"," 'fraudTrain.csv',\n"," 'Fraudulent transactions-Bkp02Sept2024.ipynb',\n"," 'Fraudulent transactions-Copy1.ipynb',\n"," 'Fraudulent transactions.ipynb',\n"," 'fraud_detection_nn.pth',\n"," 'fraud_detection_nn_es.pth',\n"," 'LIME_implementation',\n"," 'LIME_implementation.zip',\n"," 'logs',\n"," 'notebook42ad1c6289.ipynb',\n"," 'orders.csv',\n"," 'orders_updated.csv',\n"," 'proj2',\n"," 'results',\n"," 'tableqna (2).ipynb',\n"," 'tableqna_latest.ipynb',\n"," 'Tapas',\n"," 'Tapas.ipynb',\n"," 'tokenized_data.pt',\n"," 'train_df.csv',\n"," 'train_df_new (1).csv',\n"," 'train_df_new.csv',\n"," 'Untitled.ipynb']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"7ec0dd82","cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Check for missing values\nmissing_values = order_data.isnull().sum()\nprint(\"Missing values in each column:\\n\", missing_values)\n\n# Check for duplicate rows\nduplicate_rows = order_data.duplicated().sum()\nprint(\"Number of duplicate rows:\", duplicate_rows)\n\n# Optionally, remove duplicate rows\norder_data = order_data.drop_duplicates()\n\n# Check data types\ndata_types = order_data.dtypes\nprint(\"Data types of each column:\\n\", data_types)\n\n# Generate basic statistics\nstatistics = order_data.describe()\nprint(\"Basic statistics of the dataset:\\n\", statistics)\n\ncurrent_length = len(order_data)\nprint(f\"Current length of the dataset: {current_length}\")\n\norder_data=order_data.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:51:13.800202Z","iopub.execute_input":"2024-12-31T18:51:13.800588Z","iopub.status.idle":"2024-12-31T18:51:13.874685Z","shell.execute_reply.started":"2024-12-31T18:51:13.800552Z","shell.execute_reply":"2024-12-31T18:51:13.873665Z"}},"outputs":[{"name":"stdout","text":"Missing values in each column:\n Row ID             0\nOrder ID           0\nOrder Date         0\nShip Date          0\nShip Mode          0\nCustomer ID        0\nCustomer Name      0\nSegment            0\nCountry/Region     0\nCity               0\nState              0\nPostal Code       11\nRegion             0\nProduct ID         0\nCategory           0\nSub-Category       0\nProduct Name       0\nSales              0\nQuantity           0\nDiscount           0\nProfit             0\ndtype: int64\nNumber of duplicate rows: 0\nData types of each column:\n Row ID              int64\nOrder ID           object\nOrder Date         object\nShip Date          object\nShip Mode          object\nCustomer ID        object\nCustomer Name      object\nSegment            object\nCountry/Region     object\nCity               object\nState              object\nPostal Code       float64\nRegion             object\nProduct ID         object\nCategory           object\nSub-Category       object\nProduct Name       object\nSales             float64\nQuantity            int64\nDiscount          float64\nProfit            float64\ndtype: object\nBasic statistics of the dataset:\n             Row ID   Postal Code         Sales     Quantity     Discount  \\\ncount  9994.000000   9983.000000   9994.000000  9994.000000  9994.000000   \nmean   4997.500000  55245.233297    229.858001     3.789574     0.156203   \nstd    2885.163629  32038.715955    623.245101     2.225110     0.206452   \nmin       1.000000   1040.000000      0.444000     1.000000     0.000000   \n25%    2499.250000  23223.000000     17.280000     2.000000     0.000000   \n50%    4997.500000  57103.000000     54.490000     3.000000     0.200000   \n75%    7495.750000  90008.000000    209.940000     5.000000     0.200000   \nmax    9994.000000  99301.000000  22638.480000    14.000000     0.800000   \n\n            Profit  \ncount  9994.000000  \nmean     28.656896  \nstd     234.260108  \nmin   -6599.978000  \n25%       1.728750  \n50%       8.666500  \n75%      29.364000  \nmax    8399.976000  \nCurrent length of the dataset: 9994\n","output_type":"stream"}],"execution_count":46},{"id":"d7f79a34","cell_type":"code","source":"current_length = len(order_data)\nprint(f\"Current length of the dataset: {current_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:51:13.876305Z","iopub.execute_input":"2024-12-31T18:51:13.876665Z","iopub.status.idle":"2024-12-31T18:51:13.881764Z","shell.execute_reply.started":"2024-12-31T18:51:13.876635Z","shell.execute_reply":"2024-12-31T18:51:13.880687Z"}},"outputs":[{"name":"stdout","text":"Current length of the dataset: 9983\n","output_type":"stream"}],"execution_count":47},{"id":"c2bbbc07","cell_type":"code","source":"order_data=order_data[['Segment','Category','Sub-Category','Sales']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:51:15.568160Z","iopub.execute_input":"2024-12-31T18:51:15.568628Z","iopub.status.idle":"2024-12-31T18:51:15.574997Z","shell.execute_reply.started":"2024-12-31T18:51:15.568587Z","shell.execute_reply":"2024-12-31T18:51:15.573953Z"}},"outputs":[],"execution_count":49},{"id":"66388356-9153-4d3e-8747-18033a487d78","cell_type":"code","source":"grouped_data = order_data.groupby(['Segment', 'Category', 'Sub-Category'])['Sales'].sum().reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:52:17.580726Z","iopub.execute_input":"2024-12-31T18:52:17.581102Z","iopub.status.idle":"2024-12-31T18:52:17.590627Z","shell.execute_reply.started":"2024-12-31T18:52:17.581070Z","shell.execute_reply":"2024-12-31T18:52:17.589757Z"}},"outputs":[],"execution_count":55},{"id":"f653b9b9-b427-4846-a290-0fa5d8638651","cell_type":"code","source":"grouped_data['Sales']=round(grouped_data['Sales'],0).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:52:19.258190Z","iopub.execute_input":"2024-12-31T18:52:19.258648Z","iopub.status.idle":"2024-12-31T18:52:19.264807Z","shell.execute_reply.started":"2024-12-31T18:52:19.258606Z","shell.execute_reply":"2024-12-31T18:52:19.263758Z"}},"outputs":[],"execution_count":56},{"id":"2354bf3a","cell_type":"code","source":"grouped_data=grouped_data.astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:52:20.742564Z","iopub.execute_input":"2024-12-31T18:52:20.742946Z","iopub.status.idle":"2024-12-31T18:52:20.747549Z","shell.execute_reply.started":"2024-12-31T18:52:20.742920Z","shell.execute_reply":"2024-12-31T18:52:20.746531Z"}},"outputs":[],"execution_count":57},{"id":"d8d25b17","cell_type":"code","source":"grouped_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:52:22.486986Z","iopub.execute_input":"2024-12-31T18:52:22.487330Z","iopub.status.idle":"2024-12-31T18:52:22.499543Z","shell.execute_reply.started":"2024-12-31T18:52:22.487304Z","shell.execute_reply":"2024-12-31T18:52:22.498620Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"        Segment         Category Sub-Category   Sales\n0      Consumer        Furniture    Bookcases   68633\n1      Consumer        Furniture       Chairs  172148\n2      Consumer        Furniture  Furnishings   49620\n3      Consumer        Furniture       Tables   99934\n4      Consumer  Office Supplies   Appliances   52277\n5      Consumer  Office Supplies          Art   14252\n6      Consumer  Office Supplies      Binders  118161\n7      Consumer  Office Supplies    Envelopes    7769\n8      Consumer  Office Supplies    Fasteners    1681\n9      Consumer  Office Supplies       Labels    6709\n10     Consumer  Office Supplies        Paper   36232\n11     Consumer  Office Supplies      Storage  100492\n12     Consumer  Office Supplies     Supplies   25741\n13     Consumer       Technology  Accessories   87105\n14     Consumer       Technology      Copiers   69819\n15     Consumer       Technology     Machines   79543\n16     Consumer       Technology       Phones  169933\n17    Corporate        Furniture    Bookcases   29601\n18    Corporate        Furniture       Chairs   99141\n19    Corporate        Furniture  Furnishings   25001\n20    Corporate        Furniture       Tables   70872\n21    Corporate  Office Supplies   Appliances   36589\n22    Corporate  Office Supplies          Art    8582\n23    Corporate  Office Supplies      Binders   51560\n24    Corporate  Office Supplies    Envelopes    5943\n25    Corporate  Office Supplies    Fasteners     783\n26    Corporate  Office Supplies       Labels    4102\n27    Corporate  Office Supplies        Paper   23883\n28    Corporate  Office Supplies      Storage   78227\n29    Corporate  Office Supplies     Supplies   19435\n30    Corporate       Technology  Accessories   47886\n31    Corporate       Technology      Copiers   46829\n32    Corporate       Technology     Machines   60277\n33    Corporate       Technology       Phones   91153\n34  Home Office        Furniture    Bookcases   12241\n35  Home Office        Furniture       Chairs   56445\n36  Home Office        Furniture  Furnishings   17084\n37  Home Office        Furniture       Tables   36160\n38  Home Office  Office Supplies   Appliances   18124\n39  Home Office  Office Supplies          Art    4276\n40  Home Office  Office Supplies      Binders   33691\n41  Home Office  Office Supplies    Envelopes    2763\n42  Home Office  Office Supplies    Fasteners     560\n43  Home Office  Office Supplies       Labels    1675\n44  Home Office  Office Supplies        Paper   18272\n45  Home Office  Office Supplies      Storage   43560\n46  Home Office  Office Supplies     Supplies    1497\n47  Home Office       Technology  Accessories   32085\n48  Home Office       Technology      Copiers   32880\n49  Home Office       Technology     Machines   49419\n50  Home Office       Technology       Phones   67626","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Segment</th>\n      <th>Category</th>\n      <th>Sub-Category</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Consumer</td>\n      <td>Furniture</td>\n      <td>Bookcases</td>\n      <td>68633</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Consumer</td>\n      <td>Furniture</td>\n      <td>Chairs</td>\n      <td>172148</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consumer</td>\n      <td>Furniture</td>\n      <td>Furnishings</td>\n      <td>49620</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Consumer</td>\n      <td>Furniture</td>\n      <td>Tables</td>\n      <td>99934</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Appliances</td>\n      <td>52277</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Art</td>\n      <td>14252</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Binders</td>\n      <td>118161</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Envelopes</td>\n      <td>7769</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Fasteners</td>\n      <td>1681</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Labels</td>\n      <td>6709</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Paper</td>\n      <td>36232</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Storage</td>\n      <td>100492</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Consumer</td>\n      <td>Office Supplies</td>\n      <td>Supplies</td>\n      <td>25741</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Consumer</td>\n      <td>Technology</td>\n      <td>Accessories</td>\n      <td>87105</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Consumer</td>\n      <td>Technology</td>\n      <td>Copiers</td>\n      <td>69819</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Consumer</td>\n      <td>Technology</td>\n      <td>Machines</td>\n      <td>79543</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Consumer</td>\n      <td>Technology</td>\n      <td>Phones</td>\n      <td>169933</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Corporate</td>\n      <td>Furniture</td>\n      <td>Bookcases</td>\n      <td>29601</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Corporate</td>\n      <td>Furniture</td>\n      <td>Chairs</td>\n      <td>99141</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Corporate</td>\n      <td>Furniture</td>\n      <td>Furnishings</td>\n      <td>25001</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Corporate</td>\n      <td>Furniture</td>\n      <td>Tables</td>\n      <td>70872</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Appliances</td>\n      <td>36589</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Art</td>\n      <td>8582</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Binders</td>\n      <td>51560</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Envelopes</td>\n      <td>5943</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Fasteners</td>\n      <td>783</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Labels</td>\n      <td>4102</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Paper</td>\n      <td>23883</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Storage</td>\n      <td>78227</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Corporate</td>\n      <td>Office Supplies</td>\n      <td>Supplies</td>\n      <td>19435</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Corporate</td>\n      <td>Technology</td>\n      <td>Accessories</td>\n      <td>47886</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Corporate</td>\n      <td>Technology</td>\n      <td>Copiers</td>\n      <td>46829</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Corporate</td>\n      <td>Technology</td>\n      <td>Machines</td>\n      <td>60277</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Corporate</td>\n      <td>Technology</td>\n      <td>Phones</td>\n      <td>91153</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Home Office</td>\n      <td>Furniture</td>\n      <td>Bookcases</td>\n      <td>12241</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Home Office</td>\n      <td>Furniture</td>\n      <td>Chairs</td>\n      <td>56445</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Home Office</td>\n      <td>Furniture</td>\n      <td>Furnishings</td>\n      <td>17084</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Home Office</td>\n      <td>Furniture</td>\n      <td>Tables</td>\n      <td>36160</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Appliances</td>\n      <td>18124</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Art</td>\n      <td>4276</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Binders</td>\n      <td>33691</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Envelopes</td>\n      <td>2763</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Fasteners</td>\n      <td>560</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Labels</td>\n      <td>1675</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Paper</td>\n      <td>18272</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Storage</td>\n      <td>43560</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Home Office</td>\n      <td>Office Supplies</td>\n      <td>Supplies</td>\n      <td>1497</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Home Office</td>\n      <td>Technology</td>\n      <td>Accessories</td>\n      <td>32085</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Home Office</td>\n      <td>Technology</td>\n      <td>Copiers</td>\n      <td>32880</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Home Office</td>\n      <td>Technology</td>\n      <td>Machines</td>\n      <td>49419</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Home Office</td>\n      <td>Technology</td>\n      <td>Phones</td>\n      <td>67626</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":58},{"id":"fe2792fa","cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import TapasTokenizer, TapasForQuestionAnswering\n\n# Example table data\ndata = {\n    'Segment': ['Consumer', 'Corporate', 'Home Office'],\n    'Sales': ['1000', '1500', '2500']\n}\ntable = pd.DataFrame(data)\n\n# Example question and answer\nquestions = [\"What segment has the highest sales?\"]\nanswer_coordinates = [[(2 ,1)]]  # Coordinates for the answer cell\nanswer_texts = [\"1500\"]  # The text in the answer cell\n\n# Initialize the tokenizer and model\ntokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n\n# Tokenize the inputs\ntry:\n    inputs = tokenizer(\n        table=table,\n        queries=questions,\n        answer_coordinates=answer_coordinates,\n        answer_text=answer_texts,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\nexcept ValueError as e:\n    print(f\"An error occurred during tokenization: {e}\")\n\n# Forward pass through the model\noutputs = model(**inputs)\n\n# Process the outputs to get the answer\nlogits = outputs.logits\npredicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\nprint(f\"Predicted answer: {predicted_answer}\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'output_weights', 'column_output_weights', 'column_output_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\u013709\\AppData\\Local\\Temp\\ipykernel_11092\\2252861589.py\", line 40, in <module>\n","    predicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3523, in decode\n","    else:\n","          \n","  File \"c:\\Anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 207, in to_py_obj\n","    \"\"\"\n","        \n","  File \"c:\\Anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 166, in is_tf_tensor\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 157, in _is_tensorflow\n","    return isinstance(x, torch.Tensor)\n","    ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py\", line 45, in <module>\n","    from tensorflow._api.v2 import __internal__\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 11, in <module>\n","    from tensorflow._api.v2.__internal__ import distribute\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\", line 8, in <module>\n","    from tensorflow._api.v2.__internal__.distribute import combinations\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\", line 8, in <module>\n","    from tensorflow.python.distribute.combinations import env # line: 456\n","    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\", line 33, in <module>\n","    from tensorflow.python.distribute import collective_all_reduce_strategy\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 25, in <module>\n","    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\", line 28, in <module>\n","    from tensorflow.python.distribute import cross_device_utils\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py\", line 22, in <module>\n","    from tensorflow.python.distribute import values as value_lib\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py\", line 23, in <module>\n","    from tensorflow.python.distribute import distribute_lib\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 205, in <module>\n","    from tensorflow.python.data.ops import dataset_ops\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py\", line 21, in <module>\n","    from tensorflow.python.data import experimental\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\", line 97, in <module>\n","    from tensorflow.python.data.experimental import service\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\", line 419, in <module>\n","    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 23, in <module>\n","    from tensorflow.python.data.experimental.ops import compression_ops\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\", line 16, in <module>\n","    from tensorflow.python.data.util import structure\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 32, in <module>\n","    from tensorflow.python.ops.ragged import ragged_tensor\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\__init__.py\", line 28, in <module>\n","    from tensorflow.python.ops.ragged import ragged_tensor\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py\", line 2320, in <module>\n","    @type_spec_registry.register(\"tf.RaggedTensorSpec\")\n","     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\type_spec_registry.py\", line 59, in decorator_fn\n","    raise ValueError(\"Name %s has already been registered for class %s.%s.\" %\n","ValueError: Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec.\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n","    frames.append(self.format_record(record))\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n","    frame_info.lines, Colors, self.has_colors, lvals\n","    ^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n","    return self._sd.lines\n","           ^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n","    pieces = self.included_pieces\n","             ^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","                             ^^^^^^^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n","    return only(\n","           ^^^^^\n","  File \"c:\\Anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"execution_count":20},{"id":"e788f66b","cell_type":"code","source":"pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:25:52.434201Z","iopub.execute_input":"2024-12-31T18:25:52.434497Z","iopub.status.idle":"2024-12-31T18:25:56.468293Z","shell.execute_reply.started":"2024-12-31T18:25:52.434467Z","shell.execute_reply":"2024-12-31T18:25:56.467105Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"id":"1cd9109b-77ee-40dd-9211-065ba373bff3","cell_type":"code","source":"order_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:46:52.773278Z","iopub.execute_input":"2024-12-31T18:46:52.773691Z","iopub.status.idle":"2024-12-31T18:46:52.788686Z","shell.execute_reply.started":"2024-12-31T18:46:52.773659Z","shell.execute_reply":"2024-12-31T18:46:52.787765Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"         Segment Category Sales\ncount          3        3     3\nunique         3        3     3\ntop     Consumer  Student  1000\nfreq           1        1     1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Segment</th>\n      <th>Category</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Consumer</td>\n      <td>Student</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"id":"0ddf4b70","cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering\n\nmodel_name = \"google/tapas-base-finetuned-wtq\"\ndevice = -1  # Use -1 for CPU, or specify the GPU device number if available\n\n# Load the tokenizer and the model from the local directory\ntokenizer = TapasTokenizer.from_pretrained(model_name)\nmodel = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False)\n\n# Load the model and tokenizer into a question-answering pipeline\npipe = pipeline(\"table-question-answering\", model=model, tokenizer=tokenizer, device=device)\n\nquery = \"What is sum of sales of Segment Consumer and Corporate?\"\n\ndef get_answer_from_table(table, query):\n    # Run the table and query through the question-answering pipeline\n    answers = pipe(table=table, query=query)\n    return answers\n\n# Get the answer from the table\nanswers = get_answer_from_table(grouped_data, query)\nprint(answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T18:59:39.072961Z","iopub.execute_input":"2024-12-31T18:59:39.073382Z","iopub.status.idle":"2024-12-31T18:59:42.002160Z","shell.execute_reply.started":"2024-12-31T18:59:39.073351Z","shell.execute_reply":"2024-12-31T18:59:42.000712Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"{'answer': 'SUM > 68633, 172148, 99934, 52277, 14252, 7769, 1681, 6709, 36232, 100492, 25741, 79543, 169933, 8582, 5943, 23883, 19435, 47886, 46829, 91153', 'coordinates': [(0, 3), (1, 3), (3, 3), (4, 3), (5, 3), (7, 3), (8, 3), (9, 3), (10, 3), (11, 3), (12, 3), (15, 3), (16, 3), (22, 3), (24, 3), (27, 3), (29, 3), (30, 3), (31, 3), (33, 3)], 'cells': ['68633', '172148', '99934', '52277', '14252', '7769', '1681', '6709', '36232', '100492', '25741', '79543', '169933', '8582', '5943', '23883', '19435', '47886', '46829', '91153'], 'aggregator': 'SUM'}\n","output_type":"stream"}],"execution_count":68},{"id":"b651e66c","cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import TapasTokenizer, TapasForQuestionAnswering\nfrom transformers import TapasForQuestionAnswering, TrainingArguments, Trainer\nimport pandas as pd\n\norder_data = order_data.astype(str)\n\norder_data=order_data.head(500)\n# Tokenize the entire dataset\ntokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\nquestions = [\"What segment has the highest sales?\"] * len(order_data)\nanswer_coordinates = [[(0, 0)]] * len(order_data)  # Adjust coordinates as needed\nanswer_texts = ['Consumer'] * len(order_data)  # Adjust answers as needed\n\ninputs = tokenizer(\n    table=order_data,\n    queries=questions,\n    answer_coordinates=answer_coordinates,\n    answer_text=answer_texts,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nassert 'input_ids' in inputs and 'attention_mask' in inputs and 'token_type_ids' in inputs, \"Missing keys in inputs\"\nassert inputs['input_ids'].shape == inputs['attention_mask'].shape == inputs['token_type_ids'].shape, \"Mismatched dimensions in inputs\"\n\n# Save the tokenized data\ntorch.save(inputs, \"tokenized_data.pt\")\n\n# Custom Dataset class to load pre-tokenized data\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data\n\n    def __len__(self):\n        return self.tokenized_data['input_ids'].shape[0]\n\n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.tokenized_data.items()}\n\n# Load the tokenized data\ntokenized_data = torch.load(\"tokenized_data.pt\")\n\n# Create dataset instances\ntrain_dataset = CustomDataset(tokenized_data)\neval_dataset = CustomDataset(tokenized_data)\n\n# Print dataset sizes to verify\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Evaluation dataset size: {len(eval_dataset)}\")\n\n# Load the model\nmodel = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./fine-tuned-tapas\")\ntokenizer.save_pretrained(\"./fine-tuned-tapas\")\n","metadata":{},"outputs":[{"ename":"AssertionError","evalue":"Mismatched dimensions in inputs","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     17\u001b[0m     table\u001b[38;5;241m=\u001b[39morder_data,\n\u001b[0;32m     18\u001b[0m     queries\u001b[38;5;241m=\u001b[39mquestions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing keys in inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dimensions in inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Save the tokenized data\u001b[39;00m\n\u001b[0;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mAssertionError\u001b[0m: Mismatched dimensions in inputs"]}],"execution_count":13},{"id":"57f43ca9","cell_type":"code","source":"tokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n\n# Ask Questions\ninputs = tokenizer(\n    table=order_data,\n    queries=[\"What segment has the highest sales?\"],\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\noutputs = model(**inputs)\n\n# Process the outputs to get the answer\nlogits = outputs.logits\npredicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\nprint(f\"Predicted answer: {predicted_answer}\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"57ab26b0","cell_type":"code","source":"# Get the logits directly\nlogits = outputs.logits\n\n# Assuming logits is a 2D tensor with shape [batch_size, sequence_length]\n# Get the predicted token indices for start and end positions\nstart_logits, end_logits = logits.split(256, dim=1)  # Adjust the split size based on your model's output\n\n# Squeeze the last dimension\nstart_logits = start_logits.squeeze(-1)\nend_logits = end_logits.squeeze(-1)\n\n# Get the predicted start and end positions\nstart_index = torch.argmax(start_logits, dim=1).item()\nend_index = torch.argmax(end_logits, dim=1).item()\n\n# Decode the answer from the table\npredicted_answer = tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(inputs['input_ids'][0, start_index:end_index + 1])\n)\nprint(f\"Predicted answer: {predicted_answer}   start  {start_index}   end {end_index}\")\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted answer:    start  14   end 6\n"]}],"execution_count":199},{"id":"de97ee81","cell_type":"code","source":"inputs['input_ids']","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[  101,  2054,  6903,  2038,  1996,  3284,  4341,  1029,   102,  6903,\n","          4696,  4942,  1011,  4696,  7325,  7390,  2338, 18382,  2015,  7325,\n","          7390,  8397,  5971,  2436,  6067, 10873,  7325,  7390,  7251,  7325,\n","          2436,  6067,  5527,  7325,  7390, 23127,  7325,  2436,  6067,  2396,\n","          7325,  2974, 11640,  7325,  2436,  6067, 14187,  2545,  7325,  2436,\n","          6067, 22449,  7325,  7390,  7251,  7325,  2974, 11640,  7325,  2436,\n","          6067,  3259,  7325,  2436,  6067, 14187,  2545,  2188,  2436,  2436,\n","          6067, 22449,  2188,  2436,  2436,  6067, 14187,  2545,  7325,  2436,\n","          6067,  5527,  7325,  2436,  6067,  5527,  7325,  2436,  6067,  2396,\n","          7325,  2974, 11640,  7325,  2436,  6067, 14187,  2545,  5971,  2436,\n","          6067,  2396,  5971,  2436,  6067, 22449,  7325,  7390,  8397,  7325,\n","          7390,  7251,  7325,  2436,  6067, 14187,  2545,  7325,  2974, 16611,\n","          7325,  7390,  2338, 18382,  2015,  7325,  2436,  6067, 14187,  2545,\n","          7325,  7390, 23127,  7325,  2436,  6067, 11255,  2015,  7325,  2436,\n","          6067,  2396,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067,\n","          2396,  2188,  2436,  2436,  6067,  3259,  5971,  2974, 11640,  5971,\n","          7390, 23127,  2188,  2436,  2436,  6067, 11255,  2015,  2188,  2436,\n","          7390,  2338, 18382,  2015,  2188,  2436,  7390,  8397,  2188,  2436,\n","          2974, 11640,  5971,  2974, 11640,  5971,  2436,  6067,  5527,  5971,\n","          2436,  6067,  5527,  5971,  2974, 16611,  5971,  2436,  6067, 14187,\n","          2545,  7325,  2436,  6067,  5527,  7325,  2974, 16611,  7325,  2974,\n","         11640,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067, 10873,\n","          7325,  7390, 23127,  7325,  7390,  8397,  5971,  2436,  6067,  3435,\n","         24454,  2015,  5971,  2974, 11640,  7325,  2436,  6067,  5527,  7325,\n","          2436,  6067,  3259,  7325,  7390,  8397,  7325,  2436,  6067,  3259,\n","          7325,  2974, 16611,  7325,  2436,  6067, 14187,  2545,  7325,  2436,\n","          6067,  2396,  7325,  2974, 16611,  7325,  2436,  6067, 14187,  2545,\n","          7325,  2436,  6067,  3259,  7325,  7390, 23127,  2188,  2436,  7390,\n","          8397,  5971,  2436,  6067,  2396,  5971,  2974, 11640,  7325,  2436,\n","          6067,  3259,  7325,  2436,  6067, 14187,  2545,  7325,  2436,  6067,\n","          3259,  7325,  7390,  8397,  7325,  7390, 23127,  7325,  2436,  6067,\n","          5527,  5971,  2436,  6067, 14187,  2545,  5971,  7390, 23127,  5971,\n","          2436,  6067,  5527,  7325,  7390, 23127,  5971,  2436,  6067, 22449,\n","          5971,  2436,  6067, 14187,  2545,  7325,  2436,  6067,  2396,  7325,\n","          2436,  6067,  5527,  5971,  2436,  6067, 11255,  2015,  2188,  2436,\n","          2436,  6067,  5527,  7325,  7390,  8397,  7325,  2974, 16611,  7325,\n","          2436,  6067, 10873,  2188,  2436,  2436,  6067,  5527,  5971,  2436,\n","          6067,  2396,  5971,  2974, 11640,  5971,  2436,  6067,  3259,  7325,\n","          2436,  6067,  3259,  7325,  7390, 23127,  7325,  2436,  6067, 14187,\n","          2545,  2188,  2436,  2436,  6067, 14187,  2545,  2188,  2436,  7390,\n","         23127,  7325,  2436,  6067, 14187,  2545,  5971,  2436,  6067, 22449,\n","          2188,  2436,  2436,  6067,  3259,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]])"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"execution_count":200},{"id":"87d47896","cell_type":"code","source":"print(logits.shape)\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 512])\n"]}],"execution_count":192},{"id":"573c4019","cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import TapasTokenizer, TapasForQuestionAnswering, Trainer, TrainingArguments\nimport pandas as pd\n\n# Ensure order_data is defined and loaded\norder_data = pd.DataFrame({\n    'Segment': ['Consumer', 'Corporate', 'Home Office'],\n    'Sales': [1000, 1500, 500]\n})\n\ncurrent_length = len(order_data)\nrows_to_drop = current_length % 8\nif rows_to_drop > 0:\n    order_data = order_data[:-rows_to_drop]\n\n# Reset the index and convert all values to strings\norder_data.reset_index(drop=True, inplace=True)\norder_data = order_data.astype(str)\n\n# Save the updated dataset to a new CSV file\norder_data.to_csv(\"orders_updated.csv\", index=False)\n\n# Generate questions and answers for each row\nquestions = [\"What segment has the highest sales?\"] * len(order_data)\nanswer_coordinates = [[(0, 0)]] * len(order_data)  # Adjust coordinates as needed\nanswer_texts = [\"Consumer\"] * len(order_data)  # Adjust answers as needed\n\nassert len(questions) == len(order_data), \"Questions list length does not match order_data length\"\nassert len(answer_coordinates) == len(order_data), \"Answer coordinates list length does not match order_data length\"\nassert len(answer_texts) == len(order_data), \"Answer texts list length does not match order_data length\"\n\n# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, questions, answer_coordinates, answer_texts):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.questions = questions\n        self.answer_coordinates = answer_coordinates\n        self.answer_texts = answer_texts\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.dataframe):\n            raise IndexError(f\"Index {idx} is out of bounds for dataframe with length {len(self.dataframe)}\")\n\n        row = self.dataframe.iloc[idx]\n        table = pd.DataFrame([row])\n        print(f\"Processing row {idx}: {row}\")  # Debugging statement\n\n        try:\n            inputs = self.tokenizer(\n                table=table,\n                queries=[self.questions[idx]],\n                answer_coordinates=[self.answer_coordinates[idx]],\n                answer_text=[self.answer_texts[idx]],\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n        except Exception as e:\n            print(f\"Error during tokenization at index {idx}: {e}\")\n            raise e\n\n        return inputs\n\n# Create instances of the custom dataset\ntokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\ntrain_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\neval_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n\n# Print dataset sizes to verify\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Evaluation dataset size: {len(eval_dataset)}\")\n\n# Load the model from the local directory\nmodel = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# Initialize the Trainer with the custom DataLoader\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./fine-tuned-tapas\")\ntokenizer.save_pretrained(\"./fine-tuned-tapas\")\n\n# Load the fine-tuned model\ntokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n\n# Ask Questions\ninputs = tokenizer(\n    table=order_data,\n    queries=[\"What segment has the highest sales?\"],\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\noutputs = model(**inputs)\n\n# Process the outputs to get the answer\nlogits = outputs.logits\npredicted_answer = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(), skip_special_tokens=True)\nprint(f\"Predicted answer: {predicted_answer}\")\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 0\n","Evaluation dataset size: 0\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'column_output_bias', 'output_weights', 'column_output_weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[166], line 101\u001b[0m\n\u001b[0;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     94\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     95\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     96\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     97\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine-tuned-tapas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1551\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 1553\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_train_dataloader()\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;66;03m# Setting up training control variables:\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;66;03m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m total_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mworld_size\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:850\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    842\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_pin_memory,\n\u001b[0;32m    847\u001b[0m }\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[1;32m--> 850\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_sampler()\n\u001b[0;32m    851\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[0;32m    852\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:821\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LengthGroupedSampler(\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m    815\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[0;32m    816\u001b[0m         lengths\u001b[38;5;241m=\u001b[39mlengths,\n\u001b[0;32m    817\u001b[0m         model_input_name\u001b[38;5;241m=\u001b[39mmodel_input_name,\n\u001b[0;32m    818\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"execution_count":166},{"id":"d1c36e80","cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import TapasTokenizer, TapasForQuestionAnswering, Trainer, TrainingArguments\nimport pandas as pd\n\ncurrent_length = len(order_data)\nrows_to_drop = current_length % 8\nif rows_to_drop > 0:\n    order_data = order_data[:-rows_to_drop]\n\n# Reset the index and convert all values to strings\norder_data.reset_index(drop=True, inplace=True)\norder_data = order_data.astype(str)\n\n# Save the updated dataset to a new CSV file\norder_data.to_csv(\"orders_updated.csv\", index=False)\n\n# Generate questions and answers for each row\nquestions = [\"What segment has the highest sales?\"] \nanswer_coordinates = [[(0, 0)]]   # Adjust coordinates as needed\nanswer_texts = ['Consumer']  # Adjust answers as needed\n#assert len(questions) == len(order_data), \"Questions list length does not match order_data length\"\n#assert len(answer_coordinates) == len(order_data), \"Answer coordinates list length does not match order_data length\"\n#assert len(answer_texts) == len(order_data), \"Answer texts list length does not match order_data length\"\n# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, questions, answer_coordinates, answer_texts):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.questions = questions\n        self.answer_coordinates = answer_coordinates\n        self.answer_texts = answer_texts\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.dataframe):\n            raise IndexError(f\"Index {idx} is out of bounds for dataframe with length {len(self.dataframe)}\")\n\n        row = self.dataframe.iloc[idx]\n        table = pd.DataFrame([row])\n        print(f\"Processing row {idx}: {row}\")  # Debugging statement\n\n        try:\n            \n            inputs = self.tokenizer(\n                table=table,\n                queries=questions,\n                answer_coordinates=answer_coordinates,\n                answer_text=answer_texts,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n        except Exception as e:\n            print(f\"Error during tokenization at index {idx}: {e}\")\n            raise e\n\n        return inputs, idx\n\n# Create instances of the custom dataset\ntokenizer = TapasTokenizer.from_pretrained(\"./Tapas\")\ntrain_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\neval_dataset = CustomDataset(order_data, tokenizer, questions, answer_coordinates, answer_texts)\n\n# Print dataset sizes to verify\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Evaluation dataset size: {len(eval_dataset)}\")\n\n# Load the model from the local directory\nmodel = TapasForQuestionAnswering.from_pretrained(\"./Tapas\")\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# Initialize the Trainer with the custom DataLoader\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./fine-tuned-tapas\")\ntokenizer.save_pretrained(\"./fine-tuned-tapas\")\n\n# Load the fine-tuned model\ntokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n\n# Ask Questions\ninputs = tokenizer(table=order_data, queries=[\"What segment has the highest sales?\"], padding=\"max_length\", return_tensors=\"pt\")\noutputs = model(**inputs)\nprint(outputs)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 9976\n","Evaluation dataset size: 9976\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at ./Tapas and are newly initialized: ['output_bias', 'column_output_bias', 'output_weights', 'column_output_weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","C:\\Anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Processing row 7400: Segment                Consumer\n","Category        Office Supplies\n","Sub-Category           Supplies\n","Name: 7400, dtype: object\n","Error during tokenization at index 7400: iloc cannot enlarge its target object\n"]},{"ename":"IndexError","evalue":"iloc cannot enlarge its target object","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[173], line 94\u001b[0m\n\u001b[0;32m     86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     90\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine-tuned-tapas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1784\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1788\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[173], line 58\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, idx\n","Cell \u001b[1;32mIn[173], line 47\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[0;32m     48\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m     49\u001b[0m         queries\u001b[38;5;241m=\u001b[39mquestions,\n\u001b[0;32m     50\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m     51\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_texts,\n\u001b[0;32m     52\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:650\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(queries, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m    651\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    652\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    653\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    654\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    655\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    656\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    657\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    658\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    659\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    660\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    661\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    662\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    663\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    664\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    665\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    666\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    667\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m    672\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    673\u001b[0m         query\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    690\u001b[0m     )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:768\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m     )\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    769\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    770\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    771\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    772\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    773\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    774\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    775\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    776\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    777\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    778\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    779\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    780\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    781\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    782\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    783\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    784\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    785\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    787\u001b[0m )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:835\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     queries[idx] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m    833\u001b[0m     queries_tokens\u001b[38;5;241m.\u001b[39mappend(query_tokens)\n\u001b[1;32m--> 835\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_prepare_for_model(\n\u001b[0;32m    836\u001b[0m     table,\n\u001b[0;32m    837\u001b[0m     queries,\n\u001b[0;32m    838\u001b[0m     tokenized_table\u001b[38;5;241m=\u001b[39mtable_tokens,\n\u001b[0;32m    839\u001b[0m     queries_tokens\u001b[38;5;241m=\u001b[39mqueries_tokens,\n\u001b[0;32m    840\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    841\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    842\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    843\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    844\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    845\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    848\u001b[0m     prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    850\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    851\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    852\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:890\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[1;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[0;32m    889\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[38;5;241m=\u001b[39m example\n\u001b[1;32m--> 890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    891\u001b[0m         raw_table,\n\u001b[0;32m    892\u001b[0m         raw_query,\n\u001b[0;32m    893\u001b[0m         tokenized_table\u001b[38;5;241m=\u001b[39mtokenized_table,\n\u001b[0;32m    894\u001b[0m         query_tokens\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[0;32m    895\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coords,\n\u001b[0;32m    896\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_txt,\n\u001b[0;32m    897\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    898\u001b[0m         padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\u001b[38;5;241m.\u001b[39mvalue,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    899\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    900\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    901\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    902\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    903\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    904\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    905\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    906\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    908\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    909\u001b[0m         prev_answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    910\u001b[0m         prev_answer_text\u001b[38;5;241m=\u001b[39manswer_text[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    911\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1229\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[1;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     prev_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_answer_ids(\n\u001b[0;32m   1224\u001b[0m         column_ids, row_ids, table_data, prev_answer_text, prev_answer_coordinates\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# FIRST: parse both the table and question in terms of numeric values\u001b[39;00m\n\u001b[1;32m-> 1229\u001b[0m raw_table \u001b[38;5;241m=\u001b[39m add_numeric_table_values(raw_table)\n\u001b[0;32m   1230\u001b[0m raw_query \u001b[38;5;241m=\u001b[39m add_numeric_values_to_question(raw_query)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;66;03m# SECOND: add numeric-related features (and not parse them in these functions):\u001b[39;00m\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2826\u001b[0m, in \u001b[0;36madd_numeric_table_values\u001b[1;34m(table, min_consolidation_fraction, debug_info)\u001b[0m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_index, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m   2825\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[1;32m-> 2826\u001b[0m         table\u001b[38;5;241m.\u001b[39miloc[row_index, col_index] \u001b[38;5;241m=\u001b[39m Cell(text\u001b[38;5;241m=\u001b[39mcell)\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Third, add numeric_value attributes to these Cell objects\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_index, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns):\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:846\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    844\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m    845\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m--> 846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m    849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1550\u001b[0m, in \u001b[0;36m_iLocIndexer._has_valid_setitem_indexer\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(i):\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ax):\n\u001b[1;32m-> 1550\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mIndexError\u001b[0m: iloc cannot enlarge its target object"]}],"execution_count":173},{"id":"5da35076","cell_type":"code","source":"answer_texts[1]","metadata":{},"outputs":[{"data":{"text/plain":["'Consumer'"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"execution_count":141},{"id":"b5b55d3f","cell_type":"code","source":"order_data.iloc[7400]","metadata":{},"outputs":[{"data":{"text/plain":["Segment                Consumer\n","Category        Office Supplies\n","Sub-Category           Supplies\n","Name: 7400, dtype: object"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"execution_count":145},{"id":"1b859f03","cell_type":"code","source":"train_dataset[1]","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing row 1: Segment          Consumer\n","Category        Furniture\n","Sub-Category       Chairs\n","Name: 1, dtype: object\n","Error during tokenization at index 1: iloc cannot enlarge its target object\n"]},{"ename":"IndexError","evalue":"iloc cannot enlarge its target object","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset[\u001b[38;5;241m1\u001b[39m]\n","Cell \u001b[1;32mIn[144], line 55\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, idx\n","Cell \u001b[1;32mIn[144], line 44\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[0;32m     45\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m     46\u001b[0m         queries\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestions[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     47\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_coordinates[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     48\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_texts[\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m     49\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during tokenization at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:650\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(queries, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m    651\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    652\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    653\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    654\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    655\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    656\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    657\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    658\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    659\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    660\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    661\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    662\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    663\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    664\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    665\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    666\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    667\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m    672\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    673\u001b[0m         query\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    690\u001b[0m     )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:768\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m     )\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    769\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m    770\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m    771\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    772\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    773\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    774\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    775\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    776\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    777\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    778\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    779\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    780\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    781\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    782\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    783\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    784\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    785\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    787\u001b[0m )\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:835\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[1;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m     queries[idx] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m    833\u001b[0m     queries_tokens\u001b[38;5;241m.\u001b[39mappend(query_tokens)\n\u001b[1;32m--> 835\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_prepare_for_model(\n\u001b[0;32m    836\u001b[0m     table,\n\u001b[0;32m    837\u001b[0m     queries,\n\u001b[0;32m    838\u001b[0m     tokenized_table\u001b[38;5;241m=\u001b[39mtable_tokens,\n\u001b[0;32m    839\u001b[0m     queries_tokens\u001b[38;5;241m=\u001b[39mqueries_tokens,\n\u001b[0;32m    840\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[0;32m    841\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    842\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    843\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[0;32m    844\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    845\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m    848\u001b[0m     prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    850\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    851\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    852\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:890\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[1;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[0;32m    889\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[38;5;241m=\u001b[39m example\n\u001b[1;32m--> 890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    891\u001b[0m         raw_table,\n\u001b[0;32m    892\u001b[0m         raw_query,\n\u001b[0;32m    893\u001b[0m         tokenized_table\u001b[38;5;241m=\u001b[39mtokenized_table,\n\u001b[0;32m    894\u001b[0m         query_tokens\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[0;32m    895\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coords,\n\u001b[0;32m    896\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_txt,\n\u001b[0;32m    897\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    898\u001b[0m         padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\u001b[38;5;241m.\u001b[39mvalue,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    899\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m    900\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    901\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    902\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[0;32m    903\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    904\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    905\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m    906\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    908\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    909\u001b[0m         prev_answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    910\u001b[0m         prev_answer_text\u001b[38;5;241m=\u001b[39manswer_text[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    911\u001b[0m     )\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1229\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[1;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     prev_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_answer_ids(\n\u001b[0;32m   1224\u001b[0m         column_ids, row_ids, table_data, prev_answer_text, prev_answer_coordinates\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# FIRST: parse both the table and question in terms of numeric values\u001b[39;00m\n\u001b[1;32m-> 1229\u001b[0m raw_table \u001b[38;5;241m=\u001b[39m add_numeric_table_values(raw_table)\n\u001b[0;32m   1230\u001b[0m raw_query \u001b[38;5;241m=\u001b[39m add_numeric_values_to_question(raw_query)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;66;03m# SECOND: add numeric-related features (and not parse them in these functions):\u001b[39;00m\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2826\u001b[0m, in \u001b[0;36madd_numeric_table_values\u001b[1;34m(table, min_consolidation_fraction, debug_info)\u001b[0m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_index, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m   2825\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[1;32m-> 2826\u001b[0m         table\u001b[38;5;241m.\u001b[39miloc[row_index, col_index] \u001b[38;5;241m=\u001b[39m Cell(text\u001b[38;5;241m=\u001b[39mcell)\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Third, add numeric_value attributes to these Cell objects\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_index, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns):\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:846\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    844\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m    845\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m--> 846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m    849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n","File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1550\u001b[0m, in \u001b[0;36m_iLocIndexer._has_valid_setitem_indexer\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(i):\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ax):\n\u001b[1;32m-> 1550\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mIndexError\u001b[0m: iloc cannot enlarge its target object"]}],"execution_count":146},{"id":"3a649dc4","cell_type":"code","source":"\n# Step 5: Load the Fine-Tuned Model\ntokenizer = TapasTokenizer.from_pretrained(\"./fine-tuned-tapas\")\nmodel = TapasForQuestionAnswering.from_pretrained(\"./fine-tuned-tapas\")\n\n# Step 6: Ask Questions\n# Example question: Is the premium paid higher for males?\ninputs = tokenizer(table=table, queries=[\"what segament has highest sales?\"], padding=\"max_length\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\nprint(predicted_answer_coordinates)\n\n# Example question: What is the average premium paid by gender?\ninputs = tokenizer(table=table, queries=[\"What is the average sales by category?\"], padding=\"max_length\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_answer_coordinates = tokenizer.convert_logits_to_predictions(inputs, logits)\nprint(predicted_answer_coordinates)","metadata":{},"outputs":[],"execution_count":null},{"id":"f0585f56","cell_type":"code","source":"print(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Evaluation dataset size: {len(eval_dataset)}\")\n\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 7995\n","Evaluation dataset size: 1999\n"]}],"execution_count":18},{"id":"523b636c","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}